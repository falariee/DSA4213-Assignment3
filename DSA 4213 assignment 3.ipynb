{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3edc589b",
   "metadata": {},
   "source": [
    "## DSA4213 Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972ac46d",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf2c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load IMDb dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Minimal cleaning (remove <br /> tags)\n",
    "def clean_text(text):\n",
    "    return re.sub(r\"<br />\", \" \", text).strip()\n",
    "\n",
    "dataset = dataset.map(lambda x: {\"text\": clean_text(x[\"text\"])})\n",
    "\n",
    "# Tokenizer (shared for both BERT & DistilBERT)\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "encoded_dataset = dataset.map(tokenize, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8863ed",
   "metadata": {},
   "source": [
    "### Clean data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e3dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Load all IMDB data\n",
    "raw_dataset = load_dataset(\"imdb\", split=\"train+test\")\n",
    "print(f\"Total combined samples: {len(raw_dataset)}\")\n",
    "\n",
    "# 2. Remove duplicate texts\n",
    "unique_map = {}\n",
    "texts, labels = [], []\n",
    "\n",
    "for t, l in zip(raw_dataset[\"text\"], raw_dataset[\"label\"]):\n",
    "    if t not in unique_map:\n",
    "        unique_map[t] = l\n",
    "        texts.append(t)\n",
    "        labels.append(l)\n",
    "\n",
    "print(f\"Unique samples after deduplication: {len(texts)}\")\n",
    "\n",
    "# 3. Re-split cleanly (80 / 20 stratified)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "dataset_clean = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\"text\": train_texts, \"label\": train_labels}),\n",
    "    \"test\":  Dataset.from_dict({\"text\": test_texts,  \"label\": test_labels}),\n",
    "})\n",
    "\n",
    "# 4. Verify zero overlap\n",
    "train_set = set(dataset_clean[\"train\"][\"text\"])\n",
    "test_set  = set(dataset_clean[\"test\"][\"text\"])\n",
    "print(\"Overlap between train and test:\", len(train_set.intersection(test_set)))\n",
    "\n",
    "# 5. Save clean dataset\n",
    "dataset_clean.save_to_disk(\"./clean_imdb_dataset\")\n",
    "print(\"Clean dataset saved at ./clean_imdb_dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4531b065",
   "metadata": {},
   "source": [
    "### Simple EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd36ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Load dataset (cleaned version if available)\n",
    "# --------------------------------------------------------\n",
    "try:\n",
    "    dataset = load_from_disk(\"./clean_imdb_dataset\")\n",
    "    print(\"Loaded cleaned IMDb dataset from disk.\")\n",
    "except:\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    print(\"Loaded default IMDb dataset.\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "test_df = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 1. Label distribution (balance check)\n",
    "# --------------------------------------------------------\n",
    "train_counts = Counter(dataset[\"train\"][\"label\"])\n",
    "test_counts = Counter(dataset[\"test\"][\"label\"])\n",
    "\n",
    "print(\"\\nLabel Distribution:\")\n",
    "print(\"Train:\", train_counts)\n",
    "print(\"Test:\", test_counts)\n",
    "\n",
    "# Plot label distribution\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.barplot(x=[\"Negative\", \"Positive\"],\n",
    "            y=[train_counts[0], train_counts[1]],\n",
    "            palette=\"coolwarm\")\n",
    "plt.title(\"Label Distribution in IMDb Train Split\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 2. Review length analysis\n",
    "# --------------------------------------------------------\n",
    "train_df[\"char_length\"] = train_df[\"text\"].apply(len)\n",
    "train_df[\"word_length\"] = train_df[\"text\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"\\nCharacter length statistics:\")\n",
    "print(train_df[\"char_length\"].describe())\n",
    "\n",
    "print(\"\\nWord length statistics:\")\n",
    "print(train_df[\"word_length\"].describe())\n",
    "\n",
    "# Histogram of review lengths (in words)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(train_df[\"word_length\"], bins=60, color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.xlabel(\"Review Length (words)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"IMDb Review Word Count Distribution\")\n",
    "plt.xlim(0, 1200)  # trim long tail for visibility\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 3. Sample examples for quick inspection\n",
    "# --------------------------------------------------------\n",
    "print(\"\\nExample Positive Review:\\n\")\n",
    "print(train_df[train_df[\"label\"] == 1][\"text\"].iloc[0][:500])\n",
    "\n",
    "print(\"\\nExample Negative Review:\\n\")\n",
    "print(train_df[train_df[\"label\"] == 0][\"text\"].iloc[0][:500])\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 4. Train-test comparison summary\n",
    "# --------------------------------------------------------\n",
    "summary = {\n",
    "    \"Split\": [\"Train\", \"Test\"],\n",
    "    \"Total Samples\": [len(train_df), len(test_df)],\n",
    "    \"Positive\": [train_counts[1], test_counts[1]],\n",
    "    \"Negative\": [train_counts[0], test_counts[0]],\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"\\nDataset Summary:\")\n",
    "print(summary_df)\n",
    "\n",
    "# Optional: visualize comparison\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(data=summary_df.melt(id_vars=\"Split\", value_vars=[\"Positive\", \"Negative\"]),\n",
    "            x=\"Split\", y=\"value\", hue=\"variable\", palette=\"viridis\")\n",
    "plt.title(\"Positive vs Negative Reviews per Split\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd73f94",
   "metadata": {},
   "source": [
    "### Full finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae92992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import load_from_disk, load_dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Load IMDb Dataset\n",
    "# ------------------------------------------------------------\n",
    "try:\n",
    "    dataset = load_from_disk(\"./clean_imdb_dataset\")\n",
    "    print(\"Loaded cleaned IMDb dataset from disk.\")\n",
    "except:\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    print(\"Loaded default IMDb dataset.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Tokenizer and Encoding\n",
    "# ------------------------------------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Metrics\n",
    "# ------------------------------------------------------------\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"precision\": precision.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"],\n",
    "        \"recall\": recall.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"],\n",
    "        \"f1\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Training Arguments  (simplified for compatibility)\n",
    "# ------------------------------------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_full\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. FULL FINE-TUNING (BERT)\n",
    "# ------------------------------------------------------------\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "trainer_bert = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"].select(range(5000)),\n",
    "    eval_dataset=encoded_dataset[\"test\"].select(range(1000)),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Training BERT Full Fine-Tuning ===\")\n",
    "trainer_bert.train()\n",
    "print(\"\\n=== Evaluating BERT Full ===\")\n",
    "results_bert = trainer_bert.evaluate()\n",
    "print(results_bert)\n",
    "trainer_bert.save_model(\"./bert_full_finetuned\")\n",
    "print(\"Saved BERT model to ./bert_full_finetuned\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. FULL FINE-TUNING (DistilBERT)\n",
    "# ------------------------------------------------------------\n",
    "distil_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "trainer_distil = Trainer(\n",
    "    model=distil_model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"].select(range(5000)),\n",
    "    eval_dataset=encoded_dataset[\"test\"].select(range(1000)),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Training DistilBERT Full Fine-Tuning ===\")\n",
    "trainer_distil.train()\n",
    "print(\"\\n=== Evaluating DistilBERT Full ===\")\n",
    "results_distil = trainer_distil.evaluate()\n",
    "print(results_distil)\n",
    "trainer_distil.save_model(\"./distilbert_full_finetuned\")\n",
    "print(\"Saved DistilBERT model to ./distilbert_full_finetuned\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. Summary\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n=== Summary of Fine-Tuning Results ===\")\n",
    "print(\"BERT:\", results_bert)\n",
    "print(\"DistilBERT:\", results_distil)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7fd058",
   "metadata": {},
   "source": [
    "### LORA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23583314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import load_from_disk, load_dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Load IMDb Dataset\n",
    "# ------------------------------------------------------------\n",
    "try:\n",
    "    dataset = load_from_disk(\"./clean_imdb_dataset\")\n",
    "    print(\"Loaded cleaned IMDb dataset from disk.\")\n",
    "except:\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    print(\"Loaded default IMDb dataset.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Tokenizer and Encoding\n",
    "# ------------------------------------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Metrics\n",
    "# ------------------------------------------------------------\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"precision\": precision.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"],\n",
    "        \"recall\": recall.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"],\n",
    "        \"f1\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Training Arguments (simplified for compatibility)\n",
    "# ------------------------------------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_lora\",\n",
    "    learning_rate=2e-4,               # higher LR since LoRA trains fewer params\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs_lora\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. LoRA Fine-Tuning (BERT)\n",
    "# ------------------------------------------------------------\n",
    "lora_config_bert = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "bert_lora = get_peft_model(bert_model, lora_config_bert)\n",
    "\n",
    "trainer_bert = Trainer(\n",
    "    model=bert_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"].select(range(5000)),\n",
    "    eval_dataset=encoded_dataset[\"test\"].select(range(1000)),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Training BERT LoRA Fine-Tuning ===\")\n",
    "trainer_bert.train()\n",
    "print(\"\\n=== Evaluating BERT LoRA ===\")\n",
    "results_bert = trainer_bert.evaluate()\n",
    "print(results_bert)\n",
    "bert_lora.save_pretrained(\"./bert_lora_finetuned\")\n",
    "print(\"Saved BERT LoRA model to ./bert_lora_finetuned\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. LoRA Fine-Tuning (DistilBERT)\n",
    "# ------------------------------------------------------------\n",
    "lora_config_distil = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],   # correct layer names for DistilBERT\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "distil_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "distil_lora = get_peft_model(distil_model, lora_config_distil)\n",
    "\n",
    "trainer_distil = Trainer(\n",
    "    model=distil_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"].select(range(5000)),\n",
    "    eval_dataset=encoded_dataset[\"test\"].select(range(1000)),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Training DistilBERT LoRA Fine-Tuning ===\")\n",
    "trainer_distil.train()\n",
    "print(\"\\n=== Evaluating DistilBERT LoRA ===\")\n",
    "results_distil = trainer_distil.evaluate()\n",
    "print(results_distil)\n",
    "distil_lora.save_pretrained(\"./distilbert_lora_finetuned\")\n",
    "print(\"Saved DistilBERT LoRA model to ./distilbert_lora_finetuned\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. Summary\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n=== Summary of LoRA Fine-Tuning Results ===\")\n",
    "print(\"BERT LoRA:\", results_bert)\n",
    "print(\"DistilBERT LoRA:\", results_distil)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d967c6d1",
   "metadata": {},
   "source": [
    "### Prompt Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4376a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# PROMPT TUNING for BERT and DistilBERT (Final Fixed Version)\n",
    "# ------------------------------------------------------------\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import PromptTuningConfig, get_peft_model\n",
    "from datasets import load_from_disk, load_dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Load IMDb Dataset\n",
    "# ------------------------------------------------------------\n",
    "try:\n",
    "    dataset = load_from_disk(\"./clean_imdb_dataset\")\n",
    "    print(\"Loaded cleaned IMDb dataset from disk.\")\n",
    "except:\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    print(\"Loaded default IMDb dataset (may contain overlap).\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Tokenizer and Encoding\n",
    "# ------------------------------------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Metrics\n",
    "# ------------------------------------------------------------\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"precision\": precision.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"],\n",
    "        \"recall\": recall.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"],\n",
    "        \"f1\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Training Arguments\n",
    "# ------------------------------------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_prompt\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs_prompt\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Prompt Tuning - BERT\n",
    "# ------------------------------------------------------------\n",
    "prompt_config_bert = PromptTuningConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    num_virtual_tokens=20,\n",
    "    token_dim=768,              # embedding dimension\n",
    "    num_layers=12,              # BERT has 12 transformer layers\n",
    "    num_attention_heads=12      # BERT-base has 12 attention heads\n",
    ")\n",
    "\n",
    "bert_prompt = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "bert_prompt = get_peft_model(bert_prompt, prompt_config_bert)\n",
    "\n",
    "trainer_bert = Trainer(\n",
    "    model=bert_prompt,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"].select(range(5000)),\n",
    "    eval_dataset=encoded_dataset[\"test\"].select(range(1000)),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining BERT Prompt Tuning\")\n",
    "trainer_bert.train()\n",
    "print(\"\\nEvaluating BERT Prompt Tuning\")\n",
    "results_bert = trainer_bert.evaluate()\n",
    "print(results_bert)\n",
    "bert_prompt.save_pretrained(\"./bert_prompt_finetuned\")\n",
    "print(\"Saved BERT Prompt model to ./bert_prompt_finetuned\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. Prompt Tuning - DistilBERT\n",
    "# ------------------------------------------------------------\n",
    "prompt_config_distil = PromptTuningConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    num_virtual_tokens=20,\n",
    "    token_dim=768,              # embedding dimension\n",
    "    num_layers=6,               # DistilBERT has 6 layers\n",
    "    num_attention_heads=12      # DistilBERT also uses 12 heads\n",
    ")\n",
    "\n",
    "distil_prompt = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "distil_prompt = get_peft_model(distil_prompt, prompt_config_distil)\n",
    "\n",
    "trainer_distil = Trainer(\n",
    "    model=distil_prompt,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"].select(range(5000)),\n",
    "    eval_dataset=encoded_dataset[\"test\"].select(range(1000)),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining DistilBERT Prompt Tuning\")\n",
    "trainer_distil.train()\n",
    "print(\"\\nEvaluating DistilBERT Prompt Tuning\")\n",
    "results_distil = trainer_distil.evaluate()\n",
    "print(results_distil)\n",
    "distil_prompt.save_pretrained(\"./distilbert_prompt_finetuned\")\n",
    "print(\"Saved DistilBERT Prompt model to ./distilbert_prompt_finetuned\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. Summary\n",
    "# ------------------------------------------------------------\n",
    "summary_df = pd.DataFrame([\n",
    "    {\"Model\": \"BERT Prompt\", **results_bert},\n",
    "    {\"Model\": \"DistilBERT Prompt\", **results_distil}\n",
    "])\n",
    "print(\"\\nSummary of Prompt Tuning Results\")\n",
    "print(summary_df)\n",
    "summary_df.to_csv(\"prompt_tuning_results.csv\", index=False)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8. Trainable Parameters\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nTrainable Parameters for BERT Prompt:\")\n",
    "bert_prompt.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nTrainable Parameters for DistilBERT Prompt:\")\n",
    "distil_prompt.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7773971",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e98b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SENTIMENT CLASSIFICATION EVALUATION (BERT & DistilBERT Variants)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer\n",
    ")\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Load Clean IMDb Dataset\n",
    "# ------------------------------------------------------------\n",
    "dataset = load_from_disk(\"./clean_imdb_dataset\")\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Define Evaluation Metrics\n",
    "# ------------------------------------------------------------\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    pred_labels = np.argmax(preds, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=pred_labels, references=labels)[\"accuracy\"],\n",
    "        \"precision\": precision.compute(predictions=pred_labels, references=labels, average=\"macro\")[\"precision\"],\n",
    "        \"recall\": recall.compute(predictions=pred_labels, references=labels, average=\"macro\")[\"recall\"],\n",
    "        \"f1\": f1.compute(predictions=pred_labels, references=labels, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Safe Loader (Handles LoRA / Prompt / Full Models)\n",
    "# ------------------------------------------------------------\n",
    "def load_model_safely(model_path):\n",
    "    adapter_config_path = os.path.join(model_path, \"adapter_config.json\")\n",
    "    if os.path.exists(adapter_config_path):\n",
    "        print(f\"Detected adapter at {model_path}. Loading base model...\")\n",
    "        base_name = \"distilbert-base-uncased\" if \"distilbert\" in model_path.lower() else \"bert-base-uncased\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_name)\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(base_name, num_labels=2)\n",
    "        model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    else:\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        except Exception:\n",
    "            base_name = \"distilbert-base-uncased\" if \"distilbert\" in model_path.lower() else \"bert-base-uncased\"\n",
    "            tokenizer = AutoTokenizer.from_pretrained(base_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Evaluate Model & Compute Confusion Matrix\n",
    "# ------------------------------------------------------------\n",
    "def evaluate_model(model_path, max_samples=2000):\n",
    "    print(f\"\\n=== Evaluating {model_path} ===\")\n",
    "    tokenizer, model = load_model_safely(model_path)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    def preprocess(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "    \n",
    "    encoded = test_dataset.select(range(min(max_samples, len(test_dataset)))).map(preprocess, batched=True)\n",
    "    encoded = encoded.remove_columns([\"text\"])\n",
    "    encoded.set_format(\"torch\")\n",
    "\n",
    "    trainer = Trainer(model=model, tokenizer=tokenizer)\n",
    "    predictions = trainer.predict(encoded)\n",
    "\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    labels = predictions.label_ids\n",
    "    metrics = compute_metrics(predictions.predictions, labels)\n",
    "\n",
    "    cm = confusion_matrix(labels, preds, normalize=\"true\")\n",
    "\n",
    "    print(f\"Metrics for {model_path}: {metrics}\")\n",
    "    return metrics, cm\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Evaluate All Models\n",
    "# ------------------------------------------------------------\n",
    "model_folders = [\n",
    "    \"bert_full_finetuned\",\n",
    "    \"bert_lora_finetuned\",\n",
    "    \"bert_prompt_finetuned\",\n",
    "    \"distilbert_full_finetuned\",\n",
    "    \"distilbert_lora_finetuned\",\n",
    "    \"distilbert_prompt_finetuned\",\n",
    "]\n",
    "\n",
    "results, cms = [], {}\n",
    "for path in model_folders:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Skipping {path} — folder not found.\")\n",
    "        continue\n",
    "    try:\n",
    "        metrics, cm = evaluate_model(path)\n",
    "        results.append({\"Model\": path, **metrics})\n",
    "        cms[path] = cm\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {path}: {e}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. Save Quantitative Results\n",
    "# ------------------------------------------------------------\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.round(4)\n",
    "    print(\"\\n=== Evaluation Summary ===\")\n",
    "    print(results_df)\n",
    "    results_df.to_csv(\"model_evaluation_summary.csv\", index=False)\n",
    "    print(\"\\nSaved results to model_evaluation_summary.csv\")\n",
    "else:\n",
    "    print(\"\\nNo models evaluated successfully.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. Plot Confusion Matrices\n",
    "# ------------------------------------------------------------\n",
    "if cms:\n",
    "    n = len(cms)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    for i, (name, cm) in enumerate(cms.items()):\n",
    "        sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=[\"Neg\", \"Pos\"],\n",
    "                    yticklabels=[\"Neg\", \"Pos\"], ax=axes[i])\n",
    "        axes[i].set_title(name.replace(\"_\", \"\\n\"))\n",
    "        axes[i].set_xlabel(\"Predicted Label\")\n",
    "        axes[i].set_ylabel(\"True Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"confusion_matrices_all_models.png\")\n",
    "    plt.show()\n",
    "    print(\"\\nSaved confusion matrices to confusion_matrices_all_models.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689017b8",
   "metadata": {},
   "source": [
    "### Qualitative analysis examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caed5500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# QUALITATIVE ERROR EXAMPLES FROM CONFUSION MATRIX CELLS (ROBUST)\n",
    "# ==============================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "from peft import PeftModel\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 1. Load Clean IMDb Dataset\n",
    "# --------------------------------------------------------------\n",
    "dataset = load_from_disk(\"./clean_imdb_dataset\")\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2. Safe Model Loader (handles PEFT + fallback)\n",
    "# --------------------------------------------------------------\n",
    "def load_model_and_tokenizer(model_path):\n",
    "    base_name = \"distilbert-base-uncased\" if \"distilbert\" in model_path.lower() else \"bert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_name)\n",
    "\n",
    "    adapter_config_path = os.path.join(model_path, \"adapter_config.json\")\n",
    "    if os.path.exists(adapter_config_path):\n",
    "        try:\n",
    "            print(f\"🔹 Loading PEFT adapter for {model_path} ...\")\n",
    "            base_model = AutoModelForSequenceClassification.from_pretrained(base_name, num_labels=2)\n",
    "            model = PeftModel.from_pretrained(base_model, model_path)\n",
    "        except Exception as e:\n",
    "            print(f\" Failed to load PEFT model {model_path}: {e}\")\n",
    "            print(\"Falling back to standard model load instead.\")\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(base_name, num_labels=2)\n",
    "    else:\n",
    "        try:\n",
    "            print(f\"🔹 Loading full fine-tuned model for {model_path} ...\")\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {model_path} directly: {e}\")\n",
    "            print(\"Using base pretrained model instead.\")\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(base_name, num_labels=2)\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3. Extract Confusion Examples\n",
    "# --------------------------------------------------------------\n",
    "def extract_confusion_examples(model_path, max_samples=1500):\n",
    "    print(f\"\\n==================== Evaluating {model_path} ====================\")\n",
    "    tokenizer, model = load_model_and_tokenizer(model_path)\n",
    "    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def preprocess(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "    encoded = test_dataset.select(range(min(max_samples, len(test_dataset)))).map(preprocess, batched=True)\n",
    "    encoded.set_format(\"torch\")\n",
    "\n",
    "    trainer = Trainer(model=model, tokenizer=tokenizer)\n",
    "    preds_output = trainer.predict(encoded)\n",
    "    preds = np.argmax(preds_output.predictions, axis=-1)\n",
    "    labels = preds_output.label_ids\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"text\": test_dataset[\"text\"][:len(preds)],\n",
    "        \"true_label\": labels,\n",
    "        \"pred_label\": preds\n",
    "    })\n",
    "\n",
    "    df[\"true_sentiment\"] = df[\"true_label\"].map({0: \"Negative\", 1: \"Positive\"})\n",
    "    df[\"pred_sentiment\"] = df[\"pred_label\"].map({0: \"Negative\", 1: \"Positive\"})\n",
    "\n",
    "    df[\"category\"] = df.apply(lambda r:\n",
    "        \"True Positive\" if (r.true_label==1 and r.pred_label==1) else\n",
    "        \"True Negative\" if (r.true_label==0 and r.pred_label==0) else\n",
    "        \"False Positive (Neg→Pos)\" if (r.true_label==0 and r.pred_label==1) else\n",
    "        \"False Negative (Pos→Neg)\", axis=1)\n",
    "\n",
    "    errors = df[df[\"category\"].str.contains(\"False\")].reset_index(drop=True)\n",
    "    out_csv = f\"{model_path}_qualitative_errors.csv\"\n",
    "    errors.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved {len(errors)} misclassified examples to {out_csv}\")\n",
    "\n",
    "    # Show sample qualitative errors\n",
    "    for cat in [\"False Positive (Neg→Pos)\", \"False Negative (Pos→Neg)\"]:\n",
    "        subset = errors[errors[\"category\"] == cat].head(3)\n",
    "        print(f\"\\n{cat}:\")\n",
    "        for _, row in subset.iterrows():\n",
    "            preview = row['text'][:300].replace(\"\\n\", \" \")\n",
    "            print(f\"- {preview} [...] (true={row['true_sentiment']}, pred={row['pred_sentiment']})\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 4. Evaluate All Models (skip missing / broken)\n",
    "# --------------------------------------------------------------\n",
    "model_folders = [\n",
    "    \"bert_full_finetuned\",\n",
    "    \"bert_lora_finetuned\",\n",
    "    \"bert_prompt_finetuned\",\n",
    "    \"distilbert_full_finetuned\",\n",
    "    \"distilbert_lora_finetuned\",\n",
    "    \"distilbert_prompt_finetuned\",\n",
    "]\n",
    "\n",
    "for model_path in model_folders:\n",
    "    if os.path.exists(model_path):\n",
    "        try:\n",
    "            extract_confusion_examples(model_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {model_path} due to error: {e}\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"Model not found: {model_path}\")\n",
    "\n",
    "print(\"\\n==================== Evaluation Complete ====================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58b1334",
   "metadata": {},
   "source": [
    "### Toxicity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1691976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "from peft import PeftModel\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Load dataset\n",
    "# ------------------------------------------------------------\n",
    "dataset = load_from_disk(\"./clean_imdb_dataset\")\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Evaluation + save predictions\n",
    "# ------------------------------------------------------------\n",
    "def evaluate_and_save_predictions(model_path):\n",
    "    print(f\"\\nEvaluating {model_path}\")\n",
    "\n",
    "    # Detect base model\n",
    "    base_name = \"distilbert-base-uncased\" if \"distilbert\" in model_path.lower() else \"bert-base-uncased\"\n",
    "\n",
    "    # Always load tokenizer from base\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_name)\n",
    "\n",
    "    # Try loading the model\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        print(\"Loaded full fine-tuned model\")\n",
    "    except OSError:\n",
    "        print(f\"Adapter detected — loading base model '{base_name}' and merging adapter from '{model_path}'\")\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(base_name, num_labels=2)\n",
    "        model = PeftModel.from_pretrained(base_model, model_path)\n",
    "        model = model.merge_and_unload()  # optional: merge adapter into base model for evaluation\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize test set\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256\n",
    "        )\n",
    "\n",
    "    tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_test = tokenized_test.remove_columns([\"text\"])\n",
    "    tokenized_test.set_format(\"torch\")\n",
    "\n",
    "    # Run inference\n",
    "    trainer = Trainer(model=model, tokenizer=tokenizer)\n",
    "    predictions = trainer.predict(tokenized_test)\n",
    "\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    labels = predictions.label_ids\n",
    "\n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame({\n",
    "        \"text\": test_dataset[\"text\"],\n",
    "        \"true_label\": labels,\n",
    "        \"pred_label\": preds\n",
    "    })\n",
    "    df[\"true_sentiment\"] = df[\"true_label\"].map({0: \"Negative\", 1: \"Positive\"})\n",
    "    df[\"pred_sentiment\"] = df[\"pred_label\"].map({0: \"Negative\", 1: \"Positive\"})\n",
    "\n",
    "    out_path = f\"{model_path}_predictions.csv\"\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved predictions to {out_path} ({len(df)} samples)\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Evaluate all models\n",
    "# ------------------------------------------------------------\n",
    "model_folders = [\n",
    "    \"bert_lora_finetuned\",\n",
    "    \"bert_prompt_finetuned\",\n",
    "    \"distilbert_full_finetuned\",\n",
    "    \"distilbert_lora_finetuned\",\n",
    "    \"distilbert_prompt_finetuned\",\n",
    "]\n",
    "\n",
    "for path in model_folders:\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            evaluate_and_save_predictions(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {path}: {e}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Model not found: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58201477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "from peft import PeftModel\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, os, json\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Load dataset\n",
    "# ------------------------------------------------------------\n",
    "dataset = load_from_disk(\"./clean_imdb_dataset\")\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Helper: sanitize adapter_config.json (removes ANY invalid keys)\n",
    "# ------------------------------------------------------------\n",
    "def sanitize_adapter_config(model_path):\n",
    "    config_path = os.path.join(model_path, \"adapter_config.json\")\n",
    "    if not os.path.exists(config_path):\n",
    "        return\n",
    "    try:\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        valid_keys = {\n",
    "            \"base_model_name_or_path\",\n",
    "            \"peft_type\",\n",
    "            \"r\",\n",
    "            \"lora_alpha\",\n",
    "            \"target_modules\",\n",
    "            \"lora_dropout\",\n",
    "            \"bias\",\n",
    "            \"task_type\",\n",
    "            \"inference_mode\",\n",
    "            \"num_virtual_tokens\",\n",
    "            \"prompt_tuning_init\",\n",
    "            \"token_dim\",\n",
    "            \"num_transformer_submodules\",\n",
    "            \"encoder_hidden_size\",\n",
    "        }\n",
    "\n",
    "        modified = False\n",
    "        to_remove = [k for k in config.keys() if k not in valid_keys]\n",
    "\n",
    "        if to_remove:\n",
    "            print(f\"🧹 Removing unsupported keys from {config_path}: {to_remove}\")\n",
    "            for k in to_remove:\n",
    "                config.pop(k, None)\n",
    "            modified = True\n",
    "\n",
    "        if \"peft_type\" not in config:\n",
    "            if \"lora\" in model_path.lower():\n",
    "                config[\"peft_type\"] = \"LORA\"\n",
    "            elif \"prompt\" in model_path.lower():\n",
    "                config[\"peft_type\"] = \"PROMPT_TUNING\"\n",
    "            modified = True\n",
    "\n",
    "        if modified:\n",
    "            with open(config_path, \"w\") as f:\n",
    "                json.dump(config, f, indent=2)\n",
    "            print(f\"✅ Sanitized adapter config for {model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not sanitize {config_path}: {e}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Evaluation function\n",
    "# ------------------------------------------------------------\n",
    "def evaluate_and_save_predictions(model_path):\n",
    "    print(f\"\\nEvaluating {model_path}\")\n",
    "    sanitize_adapter_config(model_path)\n",
    "\n",
    "    base_name = \"distilbert-base-uncased\" if \"distilbert\" in model_path.lower() else \"bert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_name)\n",
    "\n",
    "    try:\n",
    "        print(\"⚙️ Loading base model and applying PEFT adapter...\")\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(base_name, num_labels=2)\n",
    "        model = PeftModel.from_pretrained(base_model, model_path)\n",
    "\n",
    "        # Merge for LoRA, skip for Prompt\n",
    "        try:\n",
    "            model = model.merge_and_unload()\n",
    "        except Exception:\n",
    "            print(\"ℹmerge_and_unload skipped (likely Prompt tuning)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load adapter for {model_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "    tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_test = tokenized_test.remove_columns([\"text\"])\n",
    "    tokenized_test.set_format(\"torch\")\n",
    "\n",
    "    trainer = Trainer(model=model, tokenizer=tokenizer)\n",
    "    predictions = trainer.predict(tokenized_test)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    labels = predictions.label_ids\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"text\": test_dataset[\"text\"],\n",
    "        \"true_label\": labels,\n",
    "        \"pred_label\": preds\n",
    "    })\n",
    "    df[\"true_sentiment\"] = df[\"true_label\"].map({0: \"Negative\", 1: \"Positive\"})\n",
    "    df[\"pred_sentiment\"] = df[\"pred_label\"].map({0: \"Negative\", 1: \"Positive\"})\n",
    "\n",
    "    out_path = f\"{model_path}_predictions.csv\"\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved predictions to {out_path} ({len(df)} samples)\")\n",
    "    return df\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Evaluate all models\n",
    "# ------------------------------------------------------------\n",
    "model_folders = [\n",
    "    \"bert_lora_finetuned\",\n",
    "    \"bert_prompt_finetuned\",\n",
    "    \"distilbert_lora_finetuned\",\n",
    "    \"distilbert_prompt_finetuned\",\n",
    "]\n",
    "\n",
    "for path in model_folders:\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            evaluate_and_save_predictions(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Model not found: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405263b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "from peft import PeftModel\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, os, json\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Load dataset\n",
    "# ------------------------------------------------------------\n",
    "dataset = load_from_disk(\"./clean_imdb_dataset\")\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Helper: sanitize adapter_config.json\n",
    "# ------------------------------------------------------------\n",
    "def sanitize_adapter_config(model_path):\n",
    "    config_path = os.path.join(model_path, \"adapter_config.json\")\n",
    "    if not os.path.exists(config_path):\n",
    "        return\n",
    "    try:\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        valid_keys = {\n",
    "            \"base_model_name_or_path\",\n",
    "            \"peft_type\",\n",
    "            \"r\",\n",
    "            \"lora_alpha\",\n",
    "            \"target_modules\",\n",
    "            \"lora_dropout\",\n",
    "            \"bias\",\n",
    "            \"task_type\",\n",
    "            \"inference_mode\",\n",
    "            \"num_virtual_tokens\",\n",
    "            \"prompt_tuning_init\",\n",
    "            \"token_dim\",\n",
    "            \"num_transformer_submodules\",\n",
    "            \"encoder_hidden_size\",\n",
    "            \"num_layers\",\n",
    "            \"num_attention_heads\",\n",
    "        }\n",
    "\n",
    "        modified = False\n",
    "        to_remove = [k for k in config.keys() if k not in valid_keys]\n",
    "        if to_remove:\n",
    "            print(f\"🧹 Removing unsupported keys from {config_path}: {to_remove}\")\n",
    "            for k in to_remove:\n",
    "                config.pop(k, None)\n",
    "            modified = True\n",
    "\n",
    "        if \"peft_type\" not in config:\n",
    "            if \"lora\" in model_path.lower():\n",
    "                config[\"peft_type\"] = \"LORA\"\n",
    "            elif \"prompt\" in model_path.lower():\n",
    "                config[\"peft_type\"] = \"PROMPT_TUNING\"\n",
    "            modified = True\n",
    "\n",
    "        # --- Auto-patch DistilBERT prompt tuning missing keys ---\n",
    "        if \"distilbert\" in model_path.lower() and \"prompt\" in model_path.lower():\n",
    "            if \"num_layers\" not in config:\n",
    "                config[\"num_layers\"] = 6        # DistilBERT = 6 layers\n",
    "                print(f\"Added num_layers=6 for DistilBERT prompt tuning in {model_path}\")\n",
    "                modified = True\n",
    "            if \"num_attention_heads\" not in config:\n",
    "                config[\"num_attention_heads\"] = 12   # DistilBERT = 12 heads\n",
    "                print(f\"Added num_attention_heads=12 for DistilBERT prompt tuning in {model_path}\")\n",
    "                modified = True\n",
    "\n",
    "        if modified:\n",
    "            with open(config_path, \"w\") as f:\n",
    "                json.dump(config, f, indent=2)\n",
    "            print(f\"Sanitized adapter config for {model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not sanitize {config_path}: {e}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Evaluation function\n",
    "# ------------------------------------------------------------\n",
    "def evaluate_and_save_predictions(model_path):\n",
    "    print(f\"\\nEvaluating {model_path}\")\n",
    "    sanitize_adapter_config(model_path)\n",
    "\n",
    "    base_name = \"distilbert-base-uncased\" if \"distilbert\" in model_path.lower() else \"bert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_name)\n",
    "\n",
    "    try:\n",
    "        print(\"Loading base model and applying PEFT adapter...\")\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(base_name, num_labels=2)\n",
    "        model = PeftModel.from_pretrained(base_model, model_path)\n",
    "\n",
    "        try:\n",
    "            model = model.merge_and_unload()\n",
    "        except Exception:\n",
    "            print(\"merge_and_unload skipped (likely Prompt tuning)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load adapter for {model_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "    tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_test = tokenized_test.remove_columns([\"text\"])\n",
    "    tokenized_test.set_format(\"torch\")\n",
    "\n",
    "    trainer = Trainer(model=model, tokenizer=tokenizer)\n",
    "    predictions = trainer.predict(tokenized_test)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    labels = predictions.label_ids\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"text\": test_dataset[\"text\"],\n",
    "        \"true_label\": labels,\n",
    "        \"pred_label\": preds\n",
    "    })\n",
    "    df[\"true_sentiment\"] = df[\"true_label\"].map({0: \"Negative\", 1: \"Positive\"})\n",
    "    df[\"pred_sentiment\"] = df[\"pred_label\"].map({0: \"Negative\", 1: \"Positive\"})\n",
    "\n",
    "    out_path = f\"{model_path}_predictions.csv\"\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved predictions to {out_path} ({len(df)} samples)\")\n",
    "    return df\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Evaluate all models\n",
    "# ------------------------------------------------------------\n",
    "model_folders = [\n",
    "    \"distilbert_prompt_finetuned\",\n",
    "]\n",
    "\n",
    "for path in model_folders:\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            evaluate_and_save_predictions(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {path}: {e}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Model not found: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd9ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Toxicity Analysis for Multiple Models (Clean Version)\n",
    "# ------------------------------------------------------------\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Setup\n",
    "# ------------------------------------------------------------\n",
    "MODEL_FILES = [\n",
    "    \"bert_full_finetuned_predictions.csv\",\n",
    "    \"distilbert_full_finetuned_predictions.csv\",\n",
    "    \"bert_lora_finetuned_predictions.csv\",\n",
    "    \"distilbert_lora_finetuned_predictions.csv\",\n",
    "    \"bert_prompt_finetuned_predictions.csv\",\n",
    "    \"distilbert_prompt_finetuned_predictions.csv\"\n",
    "]\n",
    "\n",
    "# Filter only existing files\n",
    "MODEL_FILES = [f for f in MODEL_FILES if os.path.exists(f)]\n",
    "assert MODEL_FILES, \"No prediction CSVs found in directory.\"\n",
    "\n",
    "print(f\"Found {len(MODEL_FILES)} prediction files to analyze.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Load Toxicity Model\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nLoading Toxicity Classifier (unitary/toxic-bert)...\")\n",
    "toxicity_pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"unitary/toxic-bert\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "def compute_toxicity(text):\n",
    "    try:\n",
    "        return toxicity_pipe(text)[0][\"score\"]\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Run Toxicity Analysis for Each Model\n",
    "# ------------------------------------------------------------\n",
    "summary = []\n",
    "\n",
    "for file in MODEL_FILES:\n",
    "    print(f\"\\nAnalyzing {file}\")\n",
    "    df = pd.read_csv(file)\n",
    "    if \"text\" not in df.columns:\n",
    "        print(f\"Skipping {file} — missing 'text' column\")\n",
    "        continue\n",
    "\n",
    "    tqdm.pandas(desc=f\"Toxicity for {file}\")\n",
    "    df[\"toxicity_score\"] = df[\"text\"].progress_apply(compute_toxicity)\n",
    "\n",
    "    # Compute overall and sentiment-based stats\n",
    "    avg_toxicity = df[\"toxicity_score\"].mean()\n",
    "    pos_tox = df.loc[df[\"pred_sentiment\"] == \"Positive\", \"toxicity_score\"].mean()\n",
    "    neg_tox = df.loc[df[\"pred_sentiment\"] == \"Negative\", \"toxicity_score\"].mean()\n",
    "\n",
    "    summary.append({\n",
    "        \"Model\": file.replace(\"_predictions.csv\", \"\"),\n",
    "        \"Avg_Toxicity\": round(avg_toxicity, 4),\n",
    "        \"Positive_Toxicity\": round(pos_tox, 4),\n",
    "        \"Negative_Toxicity\": round(neg_tox, 4),\n",
    "    })\n",
    "\n",
    "    # Save individual results\n",
    "    out_file = file.replace(\".csv\", \"_with_toxicity.csv\")\n",
    "    df.to_csv(out_file, index=False)\n",
    "    print(f\"Saved {out_file}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Create Summary Table\n",
    "# ------------------------------------------------------------\n",
    "summary_df = pd.DataFrame(summary).sort_values(\"Avg_Toxicity\", ascending=False)\n",
    "summary_df.to_csv(\"toxicity_comparison_summary.csv\", index=False)\n",
    "\n",
    "print(\"\\nToxicity Summary Across Models:\")\n",
    "print(summary_df)\n",
    "\n",
    "print(\"\\nSaved: toxicity_comparison_summary.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "insyncenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
